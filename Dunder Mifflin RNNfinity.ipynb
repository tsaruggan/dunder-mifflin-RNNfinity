{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dunder Mifflin RNNfinity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dunder Mifflin RNNfinity\n",
        "\n",
        "**Dunder Mifflin RNNfinity** is a recurrent neural network designed to generate new scripts for The Office. The model is trained using the transcripts from every episode across all 9 seasons of the show (74 hours) and can effectively generate new dialogues and similar sounding conversations between characters.\n",
        "\n",
        "This notebook provides a walkthrough of the project including the pre-processing the training data, developing the model architecture, training the model, and using the model to generate text. \n",
        "\n"
      ],
      "metadata": {
        "id": "BolJSKX_Qqqt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9u7gLQI7UTL"
      },
      "source": [
        "## Part 1: Preparing the training data\n",
        "\n",
        "The scripts from every episode of The Office has already been extracted and cleaned for training in the file, `the-office.txt`. This training data still needs to be pre-processed before it can be used to train our model on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7J5fz487Cnp"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, import the dataset from the file."
      ],
      "metadata": {
        "id": "IEVI8f6iWsJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'the-office.txt'\n",
        "file = open(filename,'rb')\n",
        "text = file.read().decode('utf8')\n",
        "file.close()"
      ],
      "metadata": {
        "id": "uMlAq11YWm1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the text by converting the strings into a numerical representation."
      ],
      "metadata": {
        "id": "SqivrZICXDUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i,u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "metadata": {
        "id": "_eNYRVcuXgil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create example input & output sequences of short lengths. For a given input sequence, the corresponding output sequence is just the input sequence shifted by a single character."
      ],
      "metadata": {
        "id": "QWHCCr6-X3kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "  \n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "G8jhat6GYhf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Produce training batches by shuffling the dataset of example sequences and packing them into batches."
      ],
      "metadata": {
        "id": "sX9ColSWY4PC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlUGW6DanlVc"
      },
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "dataset = dataset.shuffle(buffer_size)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTXnE8QY8Jqk"
      },
      "source": [
        "## Part 2: Building the model\n",
        "\n",
        "The next step is to define the RNN's model architecture. This project uses a Sequential model with the following layers:\n",
        "- an *embedding layer* that processes the input batch sequences and maps it onto 256-dimensional word embedding vector\n",
        "- stacked *LTSM layers* that train the model using back-propagation and effectively \"remember\" past data & contexts in memory\n",
        "- a *dense layer* that recieves input from the LTSM layers and returns a likelihood for each possible character to be generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJkoFr5h7D4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99986355-41b6-41d1-c29c-9464d5575df4"
      },
      "source": [
        "vocab_size = len(vocab) \n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)])\n",
        "    return model\n",
        "\n",
        "model = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           22784     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          8392704   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 89)            91225     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,753,689\n",
            "Trainable params: 13,753,689\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the model for training with the Adam optimizer and a loss function."
      ],
      "metadata": {
        "id": "Etksqas0jv0A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo-dnPOq7D8X"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "    \n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Training the model\n",
        "\n",
        "Train the model across 50 epochs and save checkpoints to memory. This may take a few hours, so maybe watch The Office while you wait."
      ],
      "metadata": {
        "id": "PBAmfV5gk8t3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "likNZjha7EF_"
      },
      "source": [
        "!mkdir -p training\n",
        "checkpoint_dir = './training'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,\"checkpoint_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "epochs = 50\n",
        "history = model.fit(dataset, epochs=epochs, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rebuild the model and restore context from the last checkpoint. Then, save the weights to memory.\n"
      ],
      "metadata": {
        "id": "JoPJ8FUBm-iA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXWGZsfq8rVf"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(\"./training\"))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.save_weights('./training/training_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPzFX5t0-TzK"
      },
      "source": [
        "## Part 4: Generate text\n",
        "\n",
        "The model is now trained and ready to be used to generate text. Just set the following parameters:\n",
        "\n",
        "- `start_string`: str; a prompt to start with and set the context of the text generation\n",
        "- `num`: int; the number of characters to generate\n",
        "- `temp`: float; the \"temperature\" of the text generated. The lower the value, the more predictable the text will be. The higher the value, the more surprising (and random) the text will be.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIYKLBwT8rZD"
      },
      "source": [
        "def generate_text(model, start_string, num = 1000, temp = 0.50):\n",
        "    # converting start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    # empty list to store our results\n",
        "    text_generated = []\n",
        "    model.reset_states() # reset context from previous use\n",
        "    for i in range(num):\n",
        "        predictions = model(input_eval)\n",
        "        # remove batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temp\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        # pass predicted character as next input to the model along with the previous (hidden) state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model, start_string=\"JIM:\\n\", num=1000, temp=0.25))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSn-8ro9-gtK"
      },
      "source": [
        "## Part 5 (Optional): Exporting the model\n",
        "\n",
        "Export the model to be used in your application.\n",
        "\n",
        "Warning: *This step likely would require restarting the runtime and reimporting an earlier version of Tensorflow in order to use some of the deprecated functionality. Some of the previous cells may also need to be rerun to prevent errors.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmkv_jpl-rgT"
      },
      "source": [
        "%tensorflow_version 1.1x\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2hRdn5v_oms"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights('./training/training_data')\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "!mkdir -p saved_model\n",
        "tf.keras.experimental.export_saved_model(model,'python_model')\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.keras.backend.clear_session()\n",
        "new_model=tf.keras.experimental.load_from_saved_model('python_model')\n",
        "\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EppxA4Sp_oyX"
      },
      "source": [
        "!pip install tensorflowjs\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "tfjs.converters.save_keras_model(new_model,'./javascript_model/model')\n",
        "!zip -r javascript_model.zip javascript_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}