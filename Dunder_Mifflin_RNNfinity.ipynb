{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dunder-Mifflin-RNNfinity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9u7gLQI7UTL",
        "colab_type": "text"
      },
      "source": [
        "**Part 1: Preparing the training data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7J5fz487Cnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the required imports for training and fitting the model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlUGW6DanlVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Process the file and produce the training dataset\n",
        "# read the file\n",
        "filename = 'the-office.txt'\n",
        "file = open(filename,'rb')\n",
        "text = file.read().decode('utf8')\n",
        "file.close()\n",
        "\n",
        "# vectorize the text\n",
        "vocab = sorted(set(text)) # unique characters in the file\n",
        "char2idx = {u:i for i,u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# divide text into example input sequences of length:seq_length\n",
        "# corresponding example output sequences are same length excepted shifted one character\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # create training examples\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) # convert individual characters to sequences\n",
        "\n",
        "# for each sequence, form the input and output text \n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# create training batches by shuffling the data and packing it into batches\n",
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "dataset = dataset.shuffle(buffer_size)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTXnE8QY8Jqk",
        "colab_type": "text"
      },
      "source": [
        "**Part 2: Build and train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJkoFr5h7D4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the model\n",
        "vocab_size = len(vocab) \n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.LSTM(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo-dnPOq7D8X",
        "colab_type": "code",
        "outputId": "cfea52c9-9513-4f04-d41a-37012c8ad4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
        "model.summary()\n",
        "\n",
        "# get output distribution from model\n",
        "for input_example_batch, output_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch) \n",
        "\n",
        "# to get actual character indices, sample from output distribution\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "    \n",
        "model.compile(optimizer='adam', loss=loss) # attach optimizer and loss function "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           22784     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (64, None, 1024)          8392704   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 89)            91225     \n",
            "=================================================================\n",
            "Total params: 13,753,689\n",
            "Trainable params: 13,753,689\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "likNZjha7EF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "# ensure checkpoints are saved during training\n",
        "!mkdir -p training\n",
        "checkpoint_dir = './training' # directory where checkpoints will be saved\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,\"checkpoint_{epoch}\") # name of checkpoint files\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "epochs = 50\n",
        "history = model.fit(dataset, epochs=epochs, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXWGZsfq8rVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rebuild model and restore context from last checkpoint\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(\"./training\"))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()\n",
        "\n",
        "# Save the weights\n",
        "model.save_weights('./training/training_data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPzFX5t0-TzK",
        "colab_type": "text"
      },
      "source": [
        "**Part 3: Generate scripts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIYKLBwT8rZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate text\n",
        "def generate_text(model,start_string, num = 1000, temp = 0.50):\n",
        "    # converting start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "    model.reset_states() # reset context from previous use\n",
        "    for i in range(num):\n",
        "        predictions = model(input_eval)\n",
        "        # remove batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temp\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        # pass predicted character as next input to the model along with the previous (hidden) state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed4h854d-IB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(model, start_string=\"JIM:\\n\", num=1000, temp=0.25))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSn-8ro9-gtK",
        "colab_type": "text"
      },
      "source": [
        "**Part 4 (Optional): Exporting the model**\n",
        "\n",
        "*This would require restarting the runtime and reimporting an earlier version of Tensorflow in order to use some of the deprecated functionality. Some of the previous cells may need to be rerun to prevent errors.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmkv_jpl-rgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.1x\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2hRdn5v_oms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new model instance\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "# Restore the weights\n",
        "model.load_weights('./training/training_data')\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)) # attach optimizer and loss function\n",
        "\n",
        "# Save the model\n",
        "!mkdir -p saved_model\n",
        "tf.keras.experimental.export_saved_model(model,'python_model')\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.keras.backend.clear_session()\n",
        "new_model=tf.keras.experimental.load_from_saved_model('python_model')\n",
        "\n",
        "# Check its architecture\n",
        "new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EppxA4Sp_oyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the model to implement on the web using tensorflow.js\n",
        "!pip install tensorflowjs\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "tfjs.converters.save_keras_model(new_model,'./javascript_model/model')\n",
        "!zip -r javascript_model.zip javascript_model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}